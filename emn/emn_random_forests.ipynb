{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'style-table.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6422040972ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'style-table.css'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'style-notebook.css'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<style>{}</style>'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'style-table.css'"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#random Forest modeling\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "def random_forests_regerssion(dataset, target):\n",
    "    \"\"\" make predictions using Random Forest regression model. \"\"\"\n",
    "\n",
    "    estimators = range(100, 300, 20)\n",
    "    new_MSE = 0.0\n",
    "    prev_MSE = 99999.99\n",
    "    \n",
    "    #selecting best estimator\n",
    "    for eID, estimator in enumerate(estimators):\n",
    "        model = RandomForestRegressor(n_estimators=estimator,\n",
    "                                      bootstrap=True,\n",
    "                                      oob_score=True,\n",
    "                                      n_jobs=-1 ) \n",
    "        regr = model.fit(dataset, target)\n",
    "\n",
    "        f_importances = regr.feature_importances_\n",
    "        expected  = target\n",
    "        \n",
    "        predicted = regr.predict(dataset)\n",
    "        COD = regr.score(dataset , target)\n",
    "        new_MSE = mean_squared_error(predicted, expected)\n",
    "\n",
    "        if new_MSE < prev_MSE :\n",
    "            prev_MSE = best_MSE = new_MSE\n",
    "            best_COD = COD\n",
    "\n",
    "            best_features =  f_importances\n",
    "            best_estimator = estimator\n",
    "\n",
    "            imp_feat_ids = [(imp_id, imp_val) for imp_id, imp_val in enumerate (best_features)]\n",
    "            imp_feat_ids.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "            top_five_imp_feat = [item[0] for item in imp_feat_ids[:5]] \n",
    "            \n",
    "    best_features = np.asarray(best_features)\n",
    "    \n",
    "    #reduced_feat = [imp_id+1 for imp_id, imp_val in enumerate (best_features) if imp_val < np.mean(best_features)]\n",
    "    print (\"RF: \",best_COD, best_estimator, imp_feat_ids[:5])\n",
    "\n",
    "    return top_five_imp_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_final_data(ydata_final_id):\n",
    "    \"\"\" Copies the unique samples from master data file. selects one copy based in case of duplicates\"\"\"\n",
    "    xdata_final = pd.DataFrame()\n",
    "    ydata_final = pd.DataFrame()\n",
    "    \n",
    "    for idx, item in enumerate(ydata_final_id):\n",
    "        new_data = pd.DataFrame(xdata[item:item+1].values, columns=xdata.columns)\n",
    "        xdata_final = xdata_final.append(new_data)\n",
    "\n",
    "    for idy, item in enumerate(ydata_final_id):\n",
    "        new_data = pd.DataFrame(list(ydata[item:item+1].values), columns=ydata.columns)\n",
    "        ydata_final = ydata_final.append(new_data) \n",
    "    \n",
    "    return xdata_final, ydata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "def data_scaling(X_train):\n",
    "    \"\"\" Scaling  feature set\"\"\"\n",
    "\n",
    "    # Scale data\n",
    "    standard_scaler = StandardScaler()\n",
    "    Xtr_s = standard_scaler.fit_transform(X_train)\n",
    "\n",
    "    robust_scaler = RobustScaler(with_centering=False, copy=True, )\n",
    "    Xtr_r = robust_scaler.fit_transform(X_train)\n",
    "\n",
    "    return Xtr_s, Xtr_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  collections import OrderedDict\n",
    "data_final_ids = dict()\n",
    "\n",
    "\n",
    "def find_final_ids():\n",
    "    \"\"\" removes the duplicate record in featur set, \"\"\"\n",
    "    \n",
    "    global data_final_ids, xdata, ydata\n",
    "    data_final_ids = dict()\n",
    "    \n",
    "    xdata_np = np.array(xdata.values)\n",
    "    xdata_hashable =  list(map(tuple, xdata_np))\n",
    "    xdata_set = set(xdata_hashable)\n",
    "\n",
    "    data_final_ids = dict()\n",
    "    losses = ['ten', 'twen', 'threefive', 'fifty', 'sixty', 'sevenfive','nine']\n",
    "\n",
    "    for lid, loss in enumerate(losses):\n",
    "        net_dict = dict()\n",
    "        for idx, rec in enumerate(xdata_hashable):\n",
    "            net_dict.setdefault(rec, []).append(idx)\n",
    "  \n",
    "        ydata_final_id = []\n",
    "          \n",
    "        for key in net_dict:\n",
    "            if len(net_dict[key]) > 1:\n",
    "                prr_select = {}\n",
    "                for nidy in net_dict[key]:\n",
    "                    prr_select.setdefault(ydata[loss][nidy],[]).append(nidy)\n",
    "                \n",
    "                ydata_nid_dict = OrderedDict(sorted(prr_select.items()))\n",
    "                ydata_nid  = [(key, sorted(value)[0], len(value)) for (key, value) in prr_select.items()]\n",
    "                ydata_nid.sort(key=lambda tup: tup[2], reverse=True)\n",
    "                \n",
    "                #append the least of all PRR values\n",
    "                ydata_final_id.append(ydata_nid[0][1])\n",
    "            else:\n",
    "                ydata_final_id.append(net_dict[key][0])\n",
    "                \n",
    "        data_final_ids[loss] = ydata_final_id\n",
    "        #print(len(data_final_ids[loss]))#, sorted(ydata_final_id))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "def delete_feats(xdata_t, ydata_t):\n",
    "    \"\"\"deletes features with zero standerd deviation\"\"\"\n",
    "    global xdata \n",
    "       \n",
    "    matches = xdata_t.std(axis=0) > 0\n",
    "    x_selected = xdata_t[matches[matches]]\n",
    "    data_corr = np.corrcoef(x_selected)\n",
    "    data_selec = ma.masked_where(data_corr < 0.95, data_corr)\n",
    "    plt.pcolor(data_selec)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  collections import OrderedDict\n",
    "data_final_ids = dict()\n",
    "\n",
    "\n",
    "def find_final_ids_pandas(xdata, ydata, loss):\n",
    "    \"\"\" removes the duplicate record in featur set, \"\"\"\n",
    "\n",
    "    xdata_inter[loss] = ydata[loss]\n",
    "    xdata_sorted = xdata_inter.sort([loss], ascending=False)\n",
    "    cols = list(xdata_sorted.columns.values)[:-1]\n",
    "    mask = xdata_sorted.duplicated(cols, keep='first')\n",
    "    xdata_final = xdata_sorted[~mask]\n",
    "    \n",
    "    return xdata_final[~mask], xdata_final[loss]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xdata = pd.DataFrame()\n",
    "ydata = pd.DataFrame()\n",
    "\n",
    "def load_data(net_size):\n",
    "    \"\"\" loads the features(x) data and target(y_lable) data into dataframes\"\"\"\n",
    "    global xdata, ydata\n",
    "    \n",
    "    #load x_data (feeture set)\n",
    "    xdata = pd.DataFrame.from_csv('data/ecoli_%s_features.txt' %net_size, index_col=None)\n",
    "    #xdata = pd.DataFrame.from_csv('../VMNs/data/ecoli-{0}-features.txt'.format(str(net_size)), index_col=None)\n",
    "    #xdata.head()\n",
    "\n",
    "    #load target data (y label)\n",
    "    ydata = pd.DataFrame.from_csv('data/ecoli_%s_target.txt' %net_size, index_col=None)\n",
    "    #ydata = pd.DataFrame.from_csv('../VMNs/data/ecoli_{0}_target.txt'.format(str(net_size)), index_col=None)\n",
    "    #ydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(data_set, filen_name):\n",
    "    #xfile_name = '../VMNs/data/ecoli_{0}_features_no_dup.txt'.format(net_size)\n",
    "    data_Set.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Hello World!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = ['ten','twen', 'threefive', 'fifty', 'sixty', 'sevenfive', 'nine']\n",
    "net_sizes = [100]#, 200, 300, 400, 500]\n",
    "\n",
    "\n",
    "for net_size in net_sizes:\n",
    "   \n",
    "    load_data(str(net_size))\n",
    "    print(net_size)\n",
    "        \n",
    "    '''\n",
    "    for lid, loss in enumerate(losses):\n",
    "        xdata_uni, ydata_uni = find_final_ids_pandas(xdata, ydata, loss)\n",
    "        #xdata_final, ydata_final = copy_final_data(data_final_ids[loss])\n",
    "        xdata_ss , xdata_rs = data_scaling(xadata_uni)\n",
    "        #delete_feats(xdata_ss, ydata_final[loss])\n",
    "                     \n",
    "        top_five = random_forests_regerssion(xdata_ss, ydata_uni)\n",
    "       \n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF:  0.88072336844 240 [(0, 0.21727604109643289), (7, 0.13645287298088579), (1, 0.089467106287371728), (13, 0.075060396265086454), (108, 0.052224032486163474)]\n",
      "RF:  0.884175722822 280 [(0, 0.18544000780586509), (7, 0.14748399067205956), (1, 0.087336347752363874), (13, 0.077910038531908254), (2, 0.065227789425640059)]\n",
      "RF:  0.881961768624 200 [(0, 0.15570009381345554), (7, 0.10757047037815928), (13, 0.10080404977424502), (176, 0.081720287430524674), (1, 0.079247784882198732)]\n",
      "RF:  0.896557255145 220 [(61, 0.22933396149787302), (0, 0.088908812793228459), (2, 0.086675017015915998), (7, 0.079080544764304178), (108, 0.061503256024133694)]\n",
      "RF:  0.896001515991 280 [(61, 0.25863103991778386), (0, 0.12090985762772323), (13, 0.10847722019412462), (1, 0.052717748416820705), (10, 0.041293080135368351)]\n",
      "RF:  0.875610197281 260 [(61, 0.31380718362210347), (0, 0.08746330521985915), (13, 0.061419547958701852), (1, 0.04780747646384062), (19, 0.04007139393858862)]\n",
      "RF:  0.939425195278 240 [(61, 0.38924023170062372), (0, 0.050243835342985634), (13, 0.048190385778598736), (47, 0.036673944023576377), (1, 0.03537334637991265)]\n",
      "------------------------------------------------------------------------------------------------\n",
      "RF:  0.904592720548 100 [(0, 0.19613197426613696), (1, 0.12460728318431365), (124, 0.11562054592339016), (13, 0.10765027486934768), (10, 0.06460080147982028)]\n",
      "RF:  0.893147550801 180 [(0, 0.20539039873366341), (13, 0.11668140143901844), (1, 0.091776521481917686), (10, 0.061253704096794148), (91, 0.051509735116582682)]\n",
      "RF:  0.905444734129 200 [(0, 0.23976793240487948), (13, 0.1380291995080988), (91, 0.096253607246991446), (1, 0.051505997858222061), (130, 0.04806279171610079)]\n",
      "RF:  0.914439864138 140 [(91, 0.159454803938582), (13, 0.15064918712605307), (0, 0.14920551779255362), (1, 0.066007528339298627), (61, 0.050249896738220341)]\n",
      "RF:  0.923089728804 140 [(0, 0.17439015806887687), (13, 0.16902118776667874), (1, 0.12634912150362965), (91, 0.096257270439389878), (124, 0.081345210990372382)]\n",
      "RF:  0.946998463929 120 [(13, 0.22475102668606731), (130, 0.21435700349609962), (0, 0.12425713170012355), (1, 0.11970246515065684), (68, 0.069672669300994153)]\n",
      "RF:  0.977956466993 100 [(61, 0.27232055219869999), (130, 0.18619086012812161), (13, 0.11702285827029271), (0, 0.095517230337853254), (1, 0.080341769135871779)]\n",
      "------------------------------------------------------------------------------------------------\n",
      "RF:  0.93160589412 160 [(40, 0.14164213643302789), (1, 0.10845090100751491), (24, 0.10476377308694304), (7, 0.098367243273956004), (0, 0.080847078628876023)]\n",
      "RF:  0.919449847771 200 [(40, 0.10157858277360075), (24, 0.091171635096330889), (7, 0.090687396236299248), (0, 0.090386566230146279), (1, 0.077554395644449117)]\n",
      "RF:  0.917968555988 180 [(1, 0.1136968303805321), (0, 0.089641085877543325), (2, 0.067997126793736942), (21, 0.063631915296846059), (4, 0.057849002786831694)]\n",
      "RF:  0.939148719541 140 [(0, 0.11021134486834626), (75, 0.071482767244329531), (13, 0.063857750724142348), (1, 0.059009186028845535), (2, 0.054875371078341589)]\n",
      "RF:  0.931496075422 180 [(0, 0.12657051665907487), (1, 0.11073731726697288), (2, 0.060290130283321731), (13, 0.051637586178105349), (4, 0.042503826724673684)]\n",
      "RF:  0.953958805692 220 [(0, 0.14427022540368814), (124, 0.13034318973776438), (61, 0.10337073756718941), (10, 0.085049301602831001), (13, 0.075487431800866214)]\n",
      "RF:  0.984216693663 140 [(61, 0.59105188206210557), (13, 0.12955230134151863), (130, 0.08426643748667903), (124, 0.065693946002170156), (2, 0.021254115779188142)]\n",
      "------------------------------------------------------------------------------------------------\n",
      "RF:  0.92638833515 280 [(0, 0.61858573017442786), (1, 0.05571288514556378), (2, 0.048641360816559474), (13, 0.046605670113558158), (19, 0.038046301027181983)]\n",
      "RF:  0.911614141524 220 [(0, 0.50583314287549708), (1, 0.072836014353022949), (2, 0.065433697327458865), (13, 0.048987014777760123), (19, 0.037050639773835707)]\n",
      "RF:  0.915666694544 140 [(0, 0.41854753757919411), (13, 0.084134634604739508), (2, 0.069434067377591821), (1, 0.064735287698345842), (4, 0.047996629613791662)]\n",
      "RF:  0.935240513407 260 [(0, 0.39239686788937522), (1, 0.11755695384568318), (2, 0.077804743155012707), (13, 0.05431864637699748), (4, 0.03239714137255835)]\n",
      "RF:  0.933104775239 140 [(0, 0.33168767548322486), (1, 0.10717164835178403), (2, 0.0795310564116265), (13, 0.066023645929858502), (18, 0.065535170465933323)]\n",
      "RF:  0.957806991816 160 [(10, 0.14598784802718393), (0, 0.14591491242777441), (1, 0.11990469239234972), (2, 0.10211183409971045), (124, 0.077489676415324255)]\n",
      "RF:  0.977866971195 280 [(61, 0.44284392946510415), (10, 0.18356718846073206), (124, 0.11714055572188765), (2, 0.082701529014742661), (13, 0.038739334048173542)]\n",
      "------------------------------------------------------------------------------------------------\n",
      "RF:  0.891745895238 200 [(0, 0.36960400063873927), (7, 0.1579535987388791), (1, 0.13817501299085885), (2, 0.062818575293589668), (13, 0.037738881111972683)]\n",
      "RF:  0.882759114109 220 [(0, 0.18992023890417595), (7, 0.16062755025640993), (1, 0.14289373572886699), (2, 0.090378132298665953), (19, 0.076211522450320549)]\n",
      "RF:  0.88146373622 180 [(7, 0.24427614919356708), (0, 0.17133485700179585), (1, 0.13285212640623706), (2, 0.097932127083715612), (13, 0.051673841102745449)]\n",
      "RF:  0.917231159503 140 [(0, 0.21010672736314803), (51, 0.14656873483645105), (1, 0.12837700807687449), (7, 0.099210610785302022), (2, 0.090607587494892408)]\n",
      "RF:  0.941382437809 160 [(0, 0.15864115528279651), (2, 0.12134394159913767), (51, 0.11544028702087768), (61, 0.11177868793828694), (1, 0.078315268649119668)]\n",
      "RF:  0.95855453344 200 [(61, 0.45344677354525581), (0, 0.081718492374873108), (13, 0.079023837234801553), (2, 0.061972664336319205), (10, 0.058589238409879389)]\n",
      "RF:  0.982970800701 140 [(61, 0.46662485956240857), (10, 0.15019271552002636), (0, 0.061585298955623827), (13, 0.049986536233719403), (1, 0.03903687757752277)]\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "losses = ['ten','twen', 'threefive', 'fifty', 'sixty', 'sevenfive', 'nine']\n",
    "net_sizes = [100, 200, 300, 400, 500]\n",
    "\n",
    "\n",
    "for net_size in net_sizes:\n",
    "   \n",
    "    load_data(str(net_size))\n",
    "        \n",
    "    for lid, loss in enumerate(losses):\n",
    "        xdata_uni, ydata_uni = find_final_ids_pandas(xdata, ydata, loss)\n",
    "        #xdata_final, ydata_final = copy_final_data(data_final_ids[loss])\n",
    "        xdata_ss , xdata_rs = data_scaling(xadata_uni)\n",
    "        #delete_feats(xdata_ss, ydata_final[loss])\n",
    "                     \n",
    "        top_five = random_forests_regerssion(xdata_ss, ydata_uni)\n",
    "       \n",
    "    print('------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xdata_mean = xdata.mean(axis=0).values\n",
    "xdata_std = xdata.std(axis=0).values\n",
    "xdata_var = xdata.var(axis=0).values\n",
    "feat_mean_std = [(idx, xdata_mean[idx], xdata_std[idx], xdata_var[idx]) \\\n",
    "                 for idx in range( len(xdata.columns)) if xdata_mean[idx] > 0] \n",
    "feat_mean_std.sort(key=lambda tup: tup[1], reverse= True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Number of sample in each size after duplicate removal\n",
    "\n",
    "EMNs\n",
    "------------------------------------------------------------------------------------------------\n",
    "Tuple\n",
    "129 129\n",
    "238 238\n",
    "162 162\n",
    "167 167\n",
    "156 156\n",
    "Frozenset\n",
    "124 124\n",
    "230 230\n",
    "160 160\n",
    "166 166\n",
    "156 156\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "VMNs\n",
    "------------------------------------------------------------------------------------------------\n",
    "Tuple\n",
    "783 783\n",
    "860 860\n",
    "851 851\n",
    "775 775\n",
    "915 915\n",
    "Frozenset\n",
    "722 722\n",
    "845 845\n",
    "850 850\n",
    "775 775\n",
    "915 915\n",
    "----------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
