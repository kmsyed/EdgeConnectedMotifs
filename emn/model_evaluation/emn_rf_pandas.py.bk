"""
    Find the important features using Random Forest regression
    
"""


#get_ipython().magic('matplotlib inline')
import pandas as pd
import numpy as np 
from pandas import DataFrame
import matplotlib.pyplot as plt
from collections import Counter
import collections 
import sys

#random Forest modeling
from sklearn.ensemble import RandomForestRegressor 
from sklearn.metrics import mean_squared_error


#data preprocessing
from sklearn.preprocessing import StandardScaler, RobustScaler

def data_scaling(X_train):
    """ Scaling  feature set"""

    # Standerd Scale
    standard_scaler = StandardScaler()
    Xtr_s = standard_scaler.fit_transform(X_train)

    # robust Scale 
    robust_scaler = RobustScaler(with_centering=False, copy=True, )
    Xtr_r = robust_scaler.fit_transform(X_train)

    return Xtr_s, Xtr_r

#global xdata(featureset) and ydata(target)
xdata = pd.DataFrame()
ydata = pd.DataFrame()

def load_data(net_size):
    """ loads the features(x) data and target(y_lable) data into dataframes"""
    global xdata, ydata
    
    #load x_data (feeture set)
    xdata = pd.DataFrame.from_csv('../data/ecoli_%s_features.txt' %net_size, sep=',',index_col=None)
    #print(xdata.head())

    #load target data (y label)
    ydata = pd.DataFrame.from_csv('../data/ecoli_%s_target.txt' %net_size,sep=',', index_col=None)
    #print(ydata.head())
   
from  collections import OrderedDict
data_final_ids = dict()

from sklearn.model_selection import train_test_split

def find_final_ids_pandas(xdata, ydata, loss):
    """ removes the duplicate record in featur set, """

    xdata_inter = pd.DataFrame()
    cols = list(xdata.columns.values)

    xdata_inter[cols] = xdata[cols]
    xdata_inter[loss] = ydata[loss]

    xdata_sorted = xdata_inter.sort_values([loss], ascending=False)
    
    mask = xdata_sorted.duplicated(cols[:-1], keep='first')
    xdata_final = xdata_sorted[~mask]
    ydata_final = pd.DataFrame()
    ydata_final[loss] = xdata_final[loss]
    del xdata_final[loss]

    return xdata_final, ydata_final
        
def save_file(data_set, filen_name):
    xfile_name = '../VMNs/data/ecoli_{0}_features_no_dup.txt'.format(net_size)
    data_Set.to_csv(file_name, sep='\t', encoding='utf-8')

import numpy.ma as ma

def remove_highly_corr_feats(xdata_t):#, ydata_t):
    """deletes features with zero standerd deviation"""


    """    
    # calculate the correlation matrix using pearson
    df_corr = xdata_t.corr(method='pearson')
     
    # create a mask to ignore self-
    mask = np.ones(df_corr.columns.size) - np.eye(df_corr.columns.size)
    df_corr = mask * df_corr
     
    drops = []
    # loop through each variable
    for col in df_corr.columns.values:
        # if we've already determined to drop the current variable, continue
        if np.in1d([col],drops):
            continue
        
        # find all the variables that are highly correlated with the current variable 
        # and add them to the drop list 
        corr = df_corr[abs(df_corr[col]) > 0.95].index
        drops = np.union1d(drops, corr)
     
    #print( "\nDropping", drops.shape[0], "highly correlated features...\n", drops)
    xdata_t.drop(drops, axis=1, inplace=True)
    
    
    """
    # calculate the correlation matrix using spearman
    df_corr = xdata_t.corr(method='spearman')
     
    # create a mask to ignore self-
    mask = np.ones(df_corr.columns.size) - np.eye(df_corr.columns.size)
    df_corr = mask * df_corr
     
    drops = []
    # loop through each variable
    for col in df_corr.columns.values:
        # if we've already determined to drop the current variable, continue
        if np.in1d([col],drops):
            continue
        
        # find all the variables that are highly correlated with the current variable 
        # and add them to the drop list 
        corr = df_corr[abs(df_corr[col]) > 0.95].index
        drops = np.union1d(drops, corr)
     
    #print( "\nDropping", drops.shape[0], "highly correlated features...\n", drops)
    xdata_t.drop(drops, axis=1, inplace=True)
    
    xdata_t = xdata_t.drop(xdata_t.std()[xdata_t.std() == 0.0 ].index.values, axis=1)

    return xdata_t

def feature_reduction(xdata_train, xdata_test, imp_features):

    drops = []
    for fid, fval in imp_features:
        #print(fid, fval)
        #feat_name = 'feat%s' %(str(fid+1))
    
        if fval <= 0.04:
            #del xdata_old[fid]
            drops.append(fid)

    #xdata_train.drop(drops, axis=1)#, inplace=True)
    #xdata_test.drop(drops, axis=1)#, inplace=True)

    for drop in drops:
        del xdata_train[drop]
        del xdata_test[drop]

    return xdata_train, xdata_test


np.random.seed(42)

def random_forests_regerssion(x_train, y_train, x_test, y_test, feat_names):
    """ make predictions using Random Forest regression model. """

    estimators = range(100, 300, 10)
    new_MSE = 0.0
    prev_MSE = 99999.99
    best_MSE = 0.0

    new_oob_error = 0.0
    old_oob_error = 9999.99
    test_best_COD = 0.0
    test_prev_COD = -9999.0
    status = False

    #selecting best estimator
    for eID, estimator in enumerate(estimators):
        model = RandomForestRegressor(n_estimators=estimator,criterion="mse",
                                      bootstrap=True,
                                      oob_score=True,
                                      random_state=42,  
                                      max_features='sqrt',
                                      #max_depth=15,
                                      #min_samples_split=15,
                                      min_samples_leaf=5,
                                      n_jobs=-1 ) 
        
        # trai  k folds 

        regr = model.fit(x_train, y_train)

        f_importances = regr.feature_importances_

        predicted = regr.predict(x_train)

        new_MSE = mean_squared_error(predicted, y_train)
        test_COD = regr.score(x_test, y_test)


        #new_oob_error = 1 - regr.oob_score_
        #print(oob_error, new_MSE, estimator)
        if new_MSE < prev_MSE:
        #if  test_COD > test_best_COD :
        #if new_oob_error < old_oob_error :
            prev_MSE = best_MSE = new_MSE
            best_COD = regr.score(x_train, y_train)
            test_best_COD = test_prev_COD =  test_COD
            #old_oob_erro = new_oob_error
            #oob_score = regr.oob_score_
            best_features =  f_importances
            best_estimator = estimator
            status = True


    if not status :
        print('In here getting best!!')
        best_features =  f_importances
        best_estimator = estimator
        best_COD = regr.score(x_train, y_train)
        test_best_COD =  test_COD
        best_MSE = new_MSE


    imp_feat_ids = [(feat_names[imp_id], imp_val) for imp_id, imp_val in enumerate (best_features)]
    imp_feat_ids.sort(key=lambda tup: tup[1], reverse=True)

    top_five_imp_feat = [item[0] for item in imp_feat_ids[:5]] 
            
    best_features = np.asarray(best_features)
    
    #reduced_feat = [imp_id+1 for imp_id, imp_val in enumerate (best_features) if imp_val < np.mean(best_features)]
    print ("RF: ",best_COD, test_best_COD, best_estimator,  best_MSE, imp_feat_ids[:5])

    print(len([(idx, item)for idx, item in imp_feat_ids if item > 0.0]))
    return (imp_feat_ids, best_estimator)


def main():
    """ main function processing begins here"""
    global xdata, ydata

    losses = [sys.argv[2]] #['fifty']#'ten', 'twen', 'threefive', 'fifty', 'sixty', 'sevenfive', 'nine']
    net_sizes = [sys.argv[1]]#[500]

    for net_size in net_sizes:

        for lid, loss in enumerate(losses):
            load_data(str(net_size))

            '''
            ###----------------------------before feature recduction-----------------------------
            xdata_uni, ydata_uni = find_final_ids_pandas(xdata, ydata, loss)
            #xdata_uni.drop([loss], inplace=True, axis=1)
            #xdata_final, ydata_final = copy_final_data(data_final_ids[loss])
            xdata_ss , xdata_rs = data_scaling(xdata_uni)
            #delete_feats(xdata_ss, ydata_uni)
            print(net_size, loss, len(xdata_uni), len(list(xdata.columns.values)))
            feat_names = list(xdata_uni.columns.values)
            top_five, best_est = random_forests_regerssion(xdata_ss, ydata_uni[loss].values, feat_names)
            
            '''

            ###------------------------after removing correlated featurs -----------------------
            xdata_uni, ydata_uni = find_final_ids_pandas(xdata, ydata, loss)
            xdata_corr= remove_highly_corr_feats(xdata_uni)

            #print(ydata_uni[~ydata_uni.duplicated([loss], keep='last')])
            xdata_corr[loss] = ydata_uni[loss].round(0)

            #xdata_train, xdata_test = train_test_split(xdata_corr, test_size = 0.10)

            xdata_train_uni = xdata_corr[~xdata_corr.duplicated([loss], keep='last')]
            xdata_rest = xdata_corr.drop(xdata_train_uni.index)

            test_size = min(0.99, round((len(xdata_corr)*0.2)/len(xdata_rest), 2))

            #print(xdata_train_uni)

            xdata_tr, xdata_test = train_test_split(xdata_rest, test_size = test_size)

            xdata_train = pd.concat([xdata_train_uni, xdata_tr])

            xfile_name_train = '../data/train_test/ecoli_emn_{0}_{1}_train.txt'.format(net_size, loss)
            xdata_train.to_csv(xfile_name_train, sep=',', encoding='utf-8')

            xfile_name_test = '../data/train_test/ecoli_emn_{0}_{1}_test.txt'.format(net_size, loss)
            xdata_test.to_csv(xfile_name_test, sep=',', encoding='utf-8')


            """
            #print(xdata_test)
            ydata_train = pd.DataFrame()
            ydata_test = pd.DataFrame()


            ydata_train[loss] = xdata_train[loss]
            ydata_test[loss] = xdata_test[loss]

            del xdata_train[loss]
            del xdata_test[loss]

            
            #xdata_corr.to_csv('../data/pre_processed/ecoli_%s_features_no_corr_uniq.csv'%net_size, sep=',', encoding='utf-8')
            #ydata_uni.to_csv('../data/pre_processed/ecoli_%s_%s_target_no_corr_uniq.csv'%(net_size, loss), sep=',', encoding='utf-8')
            feat_names = list(xdata_train.columns.values)
            #xdata_ss , xdata_rs = data_scaling(xdata_corr)

            print(net_size, loss, len(xdata_corr), len(xdata_train),len(xdata_test), len(feat_names))
            
            top_five, best_est  = random_forests_regerssion(xdata_train, ydata_train[loss].values, xdata_test, ydata_test[loss].values, feat_names)
            #print(list(xdata.columns.values))

            
            ###----------------------------after feature recduction-----------------------------
            xdata_train_red, xdata_test_red = feature_reduction(xdata_train, xdata_test, top_five)
            feat_names = list(xdata_train_red.columns.values)
            #xdata_ss , xdata_rs = data_scaling(xdata_reduced)

            print(net_size, loss,len(xdata_corr),  len(xdata_train_red), len(xdata_test_red), len(feat_names))
            top_five = random_forests_regerssion(xdata_train_red, ydata_train[loss].values, xdata_test_red, ydata_test[loss].values, feat_names)
            """
            print('--------------------------------------------------------------------------------------------')
            
        print('#######################################################################################')

if __name__ == '__main__':
    main()
